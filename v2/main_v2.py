import argparse
from generate_error_v2 import error
import json
import pandas as pd
from tqdm import tqdm
import pickle5 as pickle
import random

def get_cluster(cluster_path):
    with open(cluster_path, "rb") as fh:
        kmeans = pickle.load(fh)
    return kmeans.labels_, kmeans.cluster_centers_

def random_error(max_length):
    current_list = list(range(max_length))
    shuffle_list = current_list.copy()
    while True:
        random.shuffle(shuffle_list)
        res = [i for i, j in zip(current_list, shuffle_list) if i == j]
        if len(res) == 0:
            break
    return shuffle_list


def gen_eda(input_file, output_file, cluster_path, error_type, K):
    labels, cluster_info = get_cluster(cluster_path)
    with open(output_file, encoding= "utf-8",mode='w+') as out:
        total_output = []
        with open(input_file) as f:
            total_data = [json.loads(line) for line in f]
        total_data = total_data[:115299]#for train set v.2.1
        df = pd.read_json(path_or_buf=input_file, lines=True)
        df = df.loc[:115298]#for train set v.2.1
        df["index"] = list(range(len(total_data)))
        df["cluster"] = labels[:115299]#for train set v.2.1
        if error_type:
            random_list = random_error(len(total_data))#used when generating random error

        for i, data in enumerate(tqdm(total_data)):
            # if i > 10:
            #     import pdb;pdb.set_trace()
            #     break
            output = {}
            output['index'] = str(i)
            output['study_id'] = data['study_id']
            output['subject_id'] = data['subject_id']
            output['findings'] = data['findings']
            output['cluster'] = str(labels[i])

            #generate error only for the impressions
            # errors generated randomly
            if error_type:
                error_impressions = total_data[random_list[i]]["impression"]
                output['impression'] = error_impressions
            else:
                #errors generated by considering clusters
                ori_impression = data["impression"]
                error_impressions, error_labels, error_clusters = error(output, df, cluster_info, K, ori_impression, data["findings"])

            if error_type:
                output['error_label'] = "random"
            
            if not error_type:
                assert len(error_impressions) != K
                current_output = output
                for impression, label, error_cluster in zip(error_impressions, error_labels, error_clusters):
                    current_output["impression"] = impression
                    current_output["error_label"] = str(label)
                    current_output["error_cluster"] = str(error_cluster)
                    json_record = json.dumps(current_output, ensure_ascii=False)
                    out.write(json_record+"\n")
            else:
                json_record = json.dumps(output, ensure_ascii=False)
                out.write(json_record+"\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, help="input file of unaugmented data")
    parser.add_argument("--cluster", default="/home/fr2zyroom/kaeunkim/noise_medical_text/dataset/KMeans500_SentTrans.pickle", type=str, help="path to cluster pickle file")
    parser.add_argument("--output", type=str, help="output file of unaugmented data")
    parser.add_argument("--random", type=bool, default=False, help="True if we want randomly generated error")
    parser.add_argument("--K", type=int, default=5, help="number of closest/farthest clusters we want")
    args = parser.parse_args()
    if args.output:
        output = args.output
    else:
        from os.path import dirname, basename, join
        output = join(dirname(args.input), 'eda_' + basename(args.input))    
    gen_eda(args.input, output, args.cluster, args.random, args.K)
    
if __name__ == "__main__":
    main()